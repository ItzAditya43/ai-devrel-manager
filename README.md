# ğŸš€ DevRel AI Assistant

Let AI analyze GitHub issues and suggest high-impact DevRel actions â€” powered by local LLMs and real-time web search.

<p align="center">
  <img src="https://img.shields.io/badge/Built%20with-Streamlit-ff4b4b?logo=streamlit&logoColor=white" alt="Streamlit Badge"/>
  <img src="https://img.shields.io/badge/License-MIT-green" alt="MIT License Badge"/>
  <img src="https://img.shields.io/badge/Status-Project%20Complete-blue" alt="Status Badge"/>
</p>

---

## ğŸ” What It Does

**DevRel AI Assistant** is an LLM-powered tool to streamline Developer Relations workflows by analyzing GitHub issues from any public repository. It provides:

- ğŸ§  **LLM-based Issue Classification** via local LLaMA 3 (Ollama)
- ğŸŒ **Web Contextual Insights** from Tavily API
- ğŸ’¡ **Concrete DevRel Suggestions** (under 120 words)
- ğŸ“Š **Interactive Dashboard** to explore, filter, and export insights
- ğŸ“¥ **Downloadable Reports** in JSON, CSV, and Markdown formats

---

## ğŸ–¼ï¸ Sample View

> Below: The dashboard showing issue classification, suggestions, and real-time GitHub insights.

![image](https://github.com/user-attachments/assets/e791b37d-4931-4f2e-982a-c76bb7e954ba)
![image](https://github.com/user-attachments/assets/689a6bde-bbde-4124-9624-0f384d26b97b)

---

## âš™ï¸ How It Works

1. **User Enters a GitHub Repo** (e.g. `langchain-ai/langchain`)
2. GitHub issues are fetched and cleaned
3. **LLaMA 3 (via Ollama)** classifies issues and suggests DevRel actions
4. **Tavily** searches the web and appends useful snippets for context
5. Dashboard shows charts, insights, and DevRel strategy suggestions

---

## ğŸ’¡ Use Cases

- Developer Relations teams prioritizing GitHub feedback
- OSS maintainers planning documentation or community outreach
- Dev Advocates summarizing user friction points
- Technical PMs managing issue triage at scale

---

## ğŸš€ Getting Started Locally

### 1. Clone the Repo
```bash
git clone https://github.com/your-username/devrel-ai-assistant.git
cd devrel-ai-assistant
````

### 2. Set Up Environment

```bash
pip install -r requirements.txt
```

### 3. Set Tavily API Key

Add this to your `.env` or `.streamlit/secrets.toml`:

```
TAVILY_API_KEY=your_key_here
```

### 4. Run Ollama LLaMA 3

```bash
ollama run llama3
```

### 5. Launch the App

```bash
streamlit run main.py
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ main.py                  # Home screen for inputting repos
â”œâ”€â”€ pages/
â”‚   â””â”€â”€ dashboard.py         # Interactive dashboard with charts & filters
â”œâ”€â”€ run_pipeline.py          # Core agent pipeline (classification + Tavily + DevRel)
â”œâ”€â”€ report_generator.py      # Generates structured reports per label
â”œâ”€â”€ report.py                # Report rendering helpers
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ classifier_agent.py  # Predicts label from issue using LLaMA
â”‚   â””â”€â”€ devrel_agent.py      # Suggests DevRel actions using LLaMA
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ github_api.py        # Pulls issues from GitHub API
â”‚   â”œâ”€â”€ github_parser.py     # Cleans/parses raw issue data
â”‚   â””â”€â”€ tavily_search.py     # Adds external context via Tavily API
â”œâ”€â”€ data/                    # Stores processed issue datasets (.json)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸŒ Deploy to Streamlit Cloud

To deploy:

* Move your `TAVILY_API_KEY` into `.streamlit/secrets.toml`
* Push to GitHub
* Deploy directly via [Streamlit Cloud](https://streamlit.io/cloud)

---

## ğŸ›¡ï¸ License

This project is licensed under the **MIT License**. Use freely, modify proudly.

---

## ğŸ™Œ Credits

Built with â¤ï¸ by \[Adi Pandey] using:

* [Streamlit](https://streamlit.io/)
* [Ollama (LLaMA 3)](https://ollama.com/)
* [Tavily](https://www.tavily.com/)

---
