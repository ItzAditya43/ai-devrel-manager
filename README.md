# ğŸš€ DevRel AI Assistant

Let AI analyze GitHub issues and suggest high-impact DevRel actions â€” powered by local LLMs and real-time web search.

<p align="center">
  <img src="https://img.shields.io/badge/Built%20with-Streamlit-ff4b4b?logo=streamlit&logoColor=white" alt="Streamlit Badge"/>
  <img src="https://img.shields.io/badge/License-MIT-green" alt="MIT License Badge"/>
  <img src="https://img.shields.io/badge/Status-Project%20Complete-blue" alt="Status Badge"/>
</p>

---

## ğŸ” What It Does

DevRel AI Assistant is an intelligent tool that automates Developer Relations workflows by analyzing GitHub issues from any public repository. It provides:

* ğŸ§  **LLM-based Issue Classification** using local LLaMA3
* ğŸŒ **Web Context Search** via Tavily snippets
* ğŸ’¡ **Actionable DevRel Suggestions** from an AI agent
* ğŸ“Š **Visual Dashboards** to explore and export insights
* ğŸ§¾ **Report Generation** with downloadable summaries

---

## ğŸ–¥ï¸ Landing Page Preview

![Landing Page Screenshot](.github/landing_screenshot.png)

---

## âš™ï¸ How It Works

1. **User Enters a GitHub Repo** (e.g., `langchain-ai/langchain`)
2. Issues are fetched and classified using a local LLaMA model
3. Tavily adds helpful web context to improve understanding
4. A DevRel agent suggests practical actions (like writing docs, starting discussions, improving README)
5. Visual dashboards and reports summarize the results

---

## ğŸ’¡ Use Cases

* Developer Advocates prioritizing GitHub feedback
* Open Source maintainers planning content or documentation
* DevRel teams wanting data-backed action suggestions
* Anyone managing large GitHub repos

---

## ğŸš€ Getting Started Locally

1. **Clone the Repo**

   ```bash
   git clone https://github.com/your-username/devrel-ai-assistant.git
   cd devrel-ai-assistant
   ```

2. **Set Up Requirements**

   ```bash
   pip install -r requirements.txt
   ```

3. **Run Locally**

   ```bash
   streamlit run main.py
   ```

4. **Make Sure Ollama Is Running with LLaMA3**

   ```bash
   ollama run llama3
   ```

---

## ğŸŒ Deployment (Streamlit Cloud)

To deploy:

* Move your secrets (like API keys) to `.streamlit/secrets.toml`
* Push the project to GitHub
* Deploy it directly via [Streamlit Community Cloud](https://streamlit.io/cloud)

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ main.py                # Landing page (SaaS-style interface)
â”œâ”€â”€ dashboard.py           # Interactive dashboard
â”œâ”€â”€ run_pipeline.py        # Core orchestration logic for processing issues
â”œâ”€â”€ report.py              # Renders downloadable summary tables in dashboard
â”œâ”€â”€ report_generator.py    # Builds structured reports per label/issue
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ classifier_agent.py   # LLaMA-based issue classification
â”‚   â”œâ”€â”€ devrel_agent.py       # DevRel suggestion generator using LLaMA
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ github_api.py         # GitHub issue fetching utility
â”‚   â”œâ”€â”€ github_parser.py      # Cleans and parses raw issue data
â”‚   â”œâ”€â”€ tavily_search.py      # Web search snippet API wrapper
â”œâ”€â”€ data/                  # Stores processed issue datasets (JSON)
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ secrets.toml       # API keys for Tavily and other services
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ›¡ï¸ License

This project is licensed under the MIT License.

---

## ğŸ™Œ Credits

Built with â¤ï¸ using [Streamlit](https://streamlit.io), [Ollama](https://ollama.com), and [Tavily](https://www.tavily.com).

---
